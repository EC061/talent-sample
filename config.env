# ==============================================================================
# SHARED CONFIGURATION (Used by both start_server and materials_description_gen.py)
# ==============================================================================

# Model configuration
MODEL_NAME="Qwen/Qwen3-VL-30B-A3B-Instruct"
# Alternative models (uncomment to use):
# MODEL_NAME="Qwen/Qwen3-VL-30B-A3B-Thinking"
# MODEL_NAME="QuantTrio/Qwen3-VL-30B-A3B-Instruct-AWQ"

# Server configuration
SERVER_HOST="0.0.0.0"
SERVER_PORT="8000"

# ==============================================================================
# VLLM SERVER SPECIFIC (start_server only)
# ==============================================================================

# Tensor parallelism (will default to GPU count if not set)
# TENSOR_PARALLEL_SIZE=""

# CUDA devices (will default to all available GPUs if not set)
# CUDA_DEVICES=""

# vLLM environment variables
VLLM_WORKER_MULTIPROC_METHOD="spawn"
VLLM_USE_FLASHINFER_MOE_FP16="1"
PYTORCH_CUDA_ALLOC_CONF="expandable_segments:True"

# vLLM model parameters
VLLM_MAX_MODEL_LEN="32768"
VLLM_MAX_NUM_SEQS="8"
VLLM_GPU_MEMORY_UTILIZATION="0.80"
VLLM_SEED="0"
VLLM_MM_ENCODER_TP_MODE="data"
VLLM_ENABLE_EXPERT_PARALLEL="true"

# ==============================================================================
# MATERIALS DESCRIPTION GENERATOR SPECIFIC (materials_description_gen.py only)
# ==============================================================================

# API client configuration
API_BASE_URL=""
API_KEY="EMPTY"
API_TIMEOUT="3600"

# Generation parameters
GENERATION_TEMPERATURE="0.6"
GENERATION_TOP_P="0.95"
GENERATION_TOP_K="20"
GENERATION_PRESENCE_PENALTY="1.5"
GENERATION_MAX_TOKENS="4096"

# Batch processing
BATCH_SIZE="10"

# File paths
MATERIALS_CSV_PATH="materials/processed_materials.csv"
MATERIALS_DIR="materials/processed"

# Prompt template (optional, will use default if not set)
# PROMPT_TEMPLATE="Your custom prompt here..."

