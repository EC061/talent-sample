#!/bin/bash
# Start vLLM server with OpenAI-compatible API for LLM models
# Usage: ./start-server-llm [options]

set -e

# Load configuration from config.yml using Python helper
SCRIPT_DIR="$(cd "$(dirname "${BASH_SOURCE[0]}")" && pwd)"
CONFIG_YML="$SCRIPT_DIR/config.yml"

if [ -f "$CONFIG_YML" ]; then
    echo "Loading LLM configuration from $CONFIG_YML"
    # Export variables derived from YAML (without persisting in env files)
    # shellcheck disable=SC2046
    eval "$(python3 "$SCRIPT_DIR/config_loader.py" llm)"
else
    echo "Warning: config.yml not found at $CONFIG_YML, using hardcoded defaults"
fi

# Detect number of available CUDA devices
if command -v nvidia-smi &> /dev/null; then
    CUDA_DEVICE_COUNT=$(nvidia-smi --query-gpu=name --format=csv,noheader | wc -l)
else
    CUDA_DEVICE_COUNT=1
    echo "Warning: nvidia-smi not found, assuming 1 GPU"
fi

# Generate default CUDA devices string (e.g., "0,1,2,3" for 4 GPUs)
DEFAULT_CUDA_DEVICES=$(seq -s, 0 $((CUDA_DEVICE_COUNT - 1)))

# Default configuration (loaded from YAML or fallback to hardcoded values)
DEFAULT_MODEL_NAME="${MODEL_NAME:-QuantTrio/Qwen3-235B-A22B-Thinking-2507-AWQ}"
DEFAULT_HOST="${SERVER_HOST:-0.0.0.0}"
DEFAULT_PORT="${SERVER_PORT:-8000}"
DEFAULT_TENSOR_PARALLEL_SIZE="${TENSOR_PARALLEL_SIZE:-$CUDA_DEVICE_COUNT}"

# Parse command line arguments
MODEL_NAME="${MODEL_NAME:-$DEFAULT_MODEL_NAME}"
HOST="${HOST:-$DEFAULT_HOST}"
PORT="${PORT:-$DEFAULT_PORT}"
TENSOR_PARALLEL_SIZE="${TENSOR_PARALLEL_SIZE:-$DEFAULT_TENSOR_PARALLEL_SIZE}"
CUDA_DEVICES="${CUDA_DEVICES:-$DEFAULT_CUDA_DEVICES}"

# Defensive: if values are literal 'None' or empty, replace with defaults
if [ "$TENSOR_PARALLEL_SIZE" = "None" ] || [ -z "$TENSOR_PARALLEL_SIZE" ]; then
    TENSOR_PARALLEL_SIZE="$DEFAULT_TENSOR_PARALLEL_SIZE"
fi
if [ "$CUDA_DEVICES" = "None" ] || [ -z "$CUDA_DEVICES" ]; then
    CUDA_DEVICES="$DEFAULT_CUDA_DEVICES"
fi

# Help function
show_help() {
    cat << EOF
Usage: $0 [options]

Start vLLM inference server with OpenAI-compatible API for LLM models

Options:
    -h, --help                    Show this help message
    -m, --model MODEL             Model name (default: $DEFAULT_MODEL_NAME)
    --host HOST                   Server host (default: $DEFAULT_HOST)
    --port PORT                   Server port (default: $DEFAULT_PORT)
    -tp, --tensor-parallel SIZE   Tensor parallel size (default: $DEFAULT_TENSOR_PARALLEL_SIZE)
    --cuda-devices DEVICES        CUDA visible devices (default: $DEFAULT_CUDA_DEVICES)

Environment Variables (can be used instead of flags):
    MODEL_NAME                    Model name
    HOST                          Server host
    PORT                          Server port
    TENSOR_PARALLEL_SIZE          Tensor parallel size
    CUDA_DEVICES                  CUDA visible devices

Examples:
    # Start server with defaults
    ./start-server-llm

    # Start with custom port
    ./start-server-llm --port 9000

    # Start with environment variables
    MODEL_NAME="QuantTrio/Qwen3-235B-A22B-Thinking-2507-AWQ" PORT=9000 ./start-server-llm

    # Use different GPUs
    ./start-server-llm --cuda-devices "0,1,2,3" -tp 4

EOF
    exit 0
}

# Parse arguments
while [[ $# -gt 0 ]]; do
    case $1 in
        -h|--help)
            show_help
            ;;
        -m|--model)
            MODEL_NAME="$2"
            shift 2
            ;;
        --host)
            HOST="$2"
            shift 2
            ;;
        --port)
            PORT="$2"
            shift 2
            ;;
        -tp|--tensor-parallel)
            TENSOR_PARALLEL_SIZE="$2"
            shift 2
            ;;
        --cuda-devices)
            CUDA_DEVICES="$2"
            shift 2
            ;;
        *)
            echo "Unknown option: $1"
            echo "Use --help for usage information"
            exit 1
            ;;
    esac
done

# Determine model path
MODEL_DIR=".models/${MODEL_NAME//\//-}"
SCRIPT_DIR="$(cd "$(dirname "${BASH_SOURCE[0]}")" && pwd)"
MODEL_PATH="$SCRIPT_DIR/$MODEL_DIR"

# Check if model exists, if not download it
if [ ! -d "$MODEL_PATH" ] || [ -z "$(ls -A "$MODEL_PATH" 2>/dev/null)" ]; then
    echo "Model not found at $MODEL_PATH"
    echo "Downloading model from Hugging Face..."
    echo ""
    
    # Create directory if it doesn't exist
    mkdir -p "$MODEL_PATH"
    
    # Download the model using Hugging Face CLI
    huggingface-cli download "$MODEL_NAME" \
        --local-dir "$MODEL_PATH" \
        --local-dir-use-symlinks False \
        --resume-download
    
    if [ $? -ne 0 ]; then
        echo ""
        echo "Error: Failed to download model"
        echo "Make sure you have huggingface-cli installed: pip install huggingface-hub"
        echo "You may also need to login: huggingface-cli login"
        exit 1
    fi
    
    echo ""
    echo "Model downloaded successfully!"
    echo ""
fi

echo "=========================================="
echo "vLLM LLM Server Configuration"
echo "=========================================="
echo "Model: $MODEL_NAME"
echo "Model Path: $MODEL_PATH"
echo "Host: $HOST"
echo "Port: $PORT"
echo "Tensor Parallel Size: $TENSOR_PARALLEL_SIZE"
echo "CUDA Devices: $CUDA_DEVICES"
echo "=========================================="
echo ""

# Set environment variables for vLLM (from YAML or defaults)
export CUDA_VISIBLE_DEVICES="$CUDA_DEVICES"
export VLLM_USE_FLASHINFER_MOE_FP16="${VLLM_USE_FLASHINFER_MOE_FP16:-1}"
# Reduce memory fragmentation (recommended by PyTorch for OOM prevention)
export PYTORCH_CUDA_ALLOC_CONF="${PYTORCH_CUDA_ALLOC_CONF:-expandable_segments:True}"

# Start vLLM server
echo "Starting vLLM LLM server..."
echo "OpenAI-compatible API will be available at: http://$HOST:$PORT/v1"
echo "Press Ctrl+C to stop the server"
echo "=========================================="
echo ""

# Build command line flags
ENABLE_EXPERT_PARALLEL_FLAG=""
if [ "${VLLM_ENABLE_EXPERT_PARALLEL:-true}" = "true" ]; then
    ENABLE_EXPERT_PARALLEL_FLAG="--enable-expert-parallel"
fi

TRUST_REMOTE_CODE_FLAG=""
if [ "${VLLM_TRUST_REMOTE_CODE:-true}" = "true" ]; then
    TRUST_REMOTE_CODE_FLAG="--trust-remote-code"
fi

DISABLE_LOG_REQUESTS_FLAG=""
if [ "${VLLM_DISABLE_LOG_REQUESTS:-true}" = "true" ]; then
    DISABLE_LOG_REQUESTS_FLAG="--disable-log-requests"
fi

# Start vLLM serve with OpenAI-compatible API (parameters from YAML or defaults)
vllm serve "$MODEL_PATH" \
    --host "$HOST" \
    --port "$PORT" \
    --served-model-name "$MODEL_NAME" \
    --tensor-parallel-size "$TENSOR_PARALLEL_SIZE" \
    $ENABLE_EXPERT_PARALLEL_FLAG \
    --swap-space "${VLLM_SWAP_SPACE:-16}" \
    --max-num-seqs "${VLLM_MAX_NUM_SEQS:-512}" \
    --max-model-len "${VLLM_MAX_MODEL_LEN:-32768}" \
    --max-seq-len-to-capture "${VLLM_MAX_SEQ_LEN_TO_CAPTURE:-32768}" \
    --gpu-memory-utilization "${VLLM_GPU_MEMORY_UTILIZATION:-0.9}" \
    --seed "${VLLM_SEED:-0}" \
    $TRUST_REMOTE_CODE_FLAG \
    $DISABLE_LOG_REQUESTS_FLAG


