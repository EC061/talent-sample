#!/bin/bash
# Start vLLM server with OpenAI-compatible API
# Usage: ./start_server [options]

set -e

# Load configuration from config.env file
SCRIPT_DIR="$(cd "$(dirname "${BASH_SOURCE[0]}")" && pwd)"
CONFIG_FILE="$SCRIPT_DIR/config.env"

if [ -f "$CONFIG_FILE" ]; then
    echo "Loading configuration from $CONFIG_FILE"
    # Export all variables from config.env
    set -a
    source "$CONFIG_FILE"
    set +a
else
    echo "Warning: config.env not found at $CONFIG_FILE, using hardcoded defaults"
fi

# Detect number of available CUDA devices
if command -v nvidia-smi &> /dev/null; then
    CUDA_DEVICE_COUNT=$(nvidia-smi --query-gpu=name --format=csv,noheader | wc -l)
else
    CUDA_DEVICE_COUNT=1
    echo "Warning: nvidia-smi not found, assuming 1 GPU"
fi

# Generate default CUDA devices string (e.g., "0,1,2,3" for 4 GPUs)
DEFAULT_CUDA_DEVICES=$(seq -s, 0 $((CUDA_DEVICE_COUNT - 1)))

# Default configuration (loaded from config.env or fallback to hardcoded values)
DEFAULT_MODEL_NAME="${MODEL_NAME:-Qwen/Qwen3-VL-30B-A3B-Instruct}"
DEFAULT_HOST="${SERVER_HOST:-0.0.0.0}"
DEFAULT_PORT="${SERVER_PORT:-8000}"
DEFAULT_TENSOR_PARALLEL_SIZE="${TENSOR_PARALLEL_SIZE:-$CUDA_DEVICE_COUNT}"

# Parse command line arguments
MODEL_NAME="${MODEL_NAME:-$DEFAULT_MODEL_NAME}"
HOST="${HOST:-$DEFAULT_HOST}"
PORT="${PORT:-$DEFAULT_PORT}"
TENSOR_PARALLEL_SIZE="${TENSOR_PARALLEL_SIZE:-$DEFAULT_TENSOR_PARALLEL_SIZE}"
CUDA_DEVICES="${CUDA_DEVICES:-$DEFAULT_CUDA_DEVICES}"

# Help function
show_help() {
    cat << EOF
Usage: $0 [options]

Start vLLM inference server with OpenAI-compatible API

Options:
    -h, --help                    Show this help message
    -m, --model MODEL             Model name (default: $DEFAULT_MODEL_NAME)
    --host HOST                   Server host (default: $DEFAULT_HOST)
    --port PORT                   Server port (default: $DEFAULT_PORT)
    -tp, --tensor-parallel SIZE   Tensor parallel size (default: $DEFAULT_TENSOR_PARALLEL_SIZE)
    --cuda-devices DEVICES        CUDA visible devices (default: $DEFAULT_CUDA_DEVICES)

Environment Variables (can be used instead of flags):
    MODEL_NAME                    Model name
    HOST                          Server host
    PORT                          Server port
    TENSOR_PARALLEL_SIZE          Tensor parallel size
    CUDA_DEVICES                  CUDA visible devices

Examples:
    # Start server with defaults
    ./start_server

    # Start with custom port
    ./start_server --port 9000

    # Start with environment variables
    MODEL_NAME="Qwen/Qwen3-VL-30B-A3B-Instruct" PORT=9000 ./start_server

    # Use different GPUs
    ./start_server --cuda-devices "0,1" -tp 2

EOF
    exit 0
}

# Parse arguments
while [[ $# -gt 0 ]]; do
    case $1 in
        -h|--help)
            show_help
            ;;
        -m|--model)
            MODEL_NAME="$2"
            shift 2
            ;;
        --host)
            HOST="$2"
            shift 2
            ;;
        --port)
            PORT="$2"
            shift 2
            ;;
        -tp|--tensor-parallel)
            TENSOR_PARALLEL_SIZE="$2"
            shift 2
            ;;
        --cuda-devices)
            CUDA_DEVICES="$2"
            shift 2
            ;;
        *)
            echo "Unknown option: $1"
            echo "Use --help for usage information"
            exit 1
            ;;
    esac
done

# Determine model path
MODEL_DIR=".models/${MODEL_NAME//\//-}"
SCRIPT_DIR="$(cd "$(dirname "${BASH_SOURCE[0]}")" && pwd)"
MODEL_PATH="$SCRIPT_DIR/$MODEL_DIR"

# Check if model exists, if not download it
if [ ! -d "$MODEL_PATH" ] || [ -z "$(ls -A "$MODEL_PATH" 2>/dev/null)" ]; then
    echo "Model not found at $MODEL_PATH"
    echo "Downloading model from Hugging Face..."
    echo ""
    
    # Create directory if it doesn't exist
    mkdir -p "$MODEL_PATH"
    
    # Download the model using Hugging Face CLI
    huggingface-cli download "$MODEL_NAME" \
        --local-dir "$MODEL_PATH" \
        --local-dir-use-symlinks False \
        --resume-download
    
    if [ $? -ne 0 ]; then
        echo ""
        echo "Error: Failed to download model"
        echo "Make sure you have huggingface-cli installed: pip install huggingface-hub"
        echo "You may also need to login: huggingface-cli login"
        exit 1
    fi
    
    echo ""
    echo "Model downloaded successfully!"
    echo ""
fi

echo "=========================================="
echo "vLLM Server Configuration"
echo "=========================================="
echo "Model: $MODEL_NAME"
echo "Model Path: $MODEL_PATH"
echo "Host: $HOST"
echo "Port: $PORT"
echo "Tensor Parallel Size: $TENSOR_PARALLEL_SIZE"
echo "CUDA Devices: $CUDA_DEVICES"
echo "=========================================="
echo ""

# Set environment variables for vLLM (from config.env or defaults)
export CUDA_VISIBLE_DEVICES="$CUDA_DEVICES"
export VLLM_WORKER_MULTIPROC_METHOD="${VLLM_WORKER_MULTIPROC_METHOD:-spawn}"
export VLLM_USE_FLASHINFER_MOE_FP16="${VLLM_USE_FLASHINFER_MOE_FP16:-1}"
export VLLM_LOGGING_LEVEL="${VLLM_LOGGING_LEVEL:-DEBUG}"
export VLLM_LOG_STATS_INTERVAL="${VLLM_LOG_STATS_INTERVAL:-1}"
# Reduce memory fragmentation (recommended by PyTorch for OOM prevention)
export PYTORCH_CUDA_ALLOC_CONF="${PYTORCH_CUDA_ALLOC_CONF:-expandable_segments:True}"

# Start vLLM server
echo "Starting vLLM server..."
echo "OpenAI-compatible API will be available at: http://$HOST:$PORT/v1"
echo "Press Ctrl+C to stop the server"
echo "=========================================="
echo ""

# Start vLLM serve with OpenAI-compatible API (parameters from config.env or defaults)
ENABLE_EXPERT_PARALLEL_FLAG=""
if [ "${VLLM_ENABLE_EXPERT_PARALLEL:-true}" = "true" ]; then
    ENABLE_EXPERT_PARALLEL_FLAG="--enable-expert-parallel"
fi

vllm serve "$MODEL_PATH" \
    --host "$HOST" \
    --port "$PORT" \
    --served-model-name "$MODEL_NAME" \
    --tensor-parallel-size "$TENSOR_PARALLEL_SIZE" \
    --mm-encoder-tp-mode "${VLLM_MM_ENCODER_TP_MODE:-data}" \
    $ENABLE_EXPERT_PARALLEL_FLAG \
    --seed "${VLLM_SEED:-0}" \
    --max-model-len "${VLLM_MAX_MODEL_LEN:-32768}" \
    --max-num-seqs "${VLLM_MAX_NUM_SEQS:-8}" \
    --gpu-memory-utilization "${VLLM_GPU_MEMORY_UTILIZATION:-0.80}" 

